{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOgUvGzoFYo1S9DP+uK1Jyr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjib7777/-Hyperspectral-Image-Classification/blob/main/indian_pipes_cuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d276mvqm3g1s",
        "outputId": "f66b4d61-7b51-4b97-f622-508537bfb3e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import dense_to_sparse\n",
        "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "XhLIQg3Z3toE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyg5K2An3yGF",
        "outputId": "b897bdf1-5671-421a-e412-ffdca00fd93f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and preprocess Indian Pines dataset\n",
        "def load_indian_pines():\n",
        "    data = sio.loadmat('Indian_pines_corrected.mat')['indian_pines_corrected']\n",
        "    labels = sio.loadmat('Indian_pines_gt.mat')['indian_pines_gt']\n",
        "    return data, labels\n",
        "\n",
        "def preprocess(data):\n",
        "    data = data.astype(np.float32)\n",
        "    data -= np.min(data)\n",
        "    data /= np.max(data)\n",
        "    return data\n",
        "\n",
        "def extract_patches(data, labels, patch_size=5):\n",
        "    margin = patch_size // 2\n",
        "    padded_data = np.pad(data, ((margin, margin), (margin, margin), (0, 0)), mode='constant')\n",
        "    h, w, c = data.shape\n",
        "    patches, targets, indices = [], [], []\n",
        "    for i in range(h):\n",
        "        for j in range(w):\n",
        "            if labels[i, j] == 0:\n",
        "                continue\n",
        "            patch = padded_data[i:i+patch_size, j:j+patch_size, :]\n",
        "            patches.append(patch)\n",
        "            targets.append(labels[i, j])\n",
        "            indices.append((i, j))\n",
        "    return np.array(patches), np.array(targets), np.array(indices)\n",
        "\n",
        "# 2. Model components\n",
        "class SpectralSpatialCNN(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv3d = nn.Sequential(\n",
        "            nn.Conv3d(1, 8, kernel_size=(3, 3, 3), padding=1),\n",
        "            nn.BatchNorm3d(8),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Conv3d(8, 16, kernel_size=(3, 3, 3), padding=1),\n",
        "            nn.BatchNorm3d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, 1, 5, 5, in_channels)\n",
        "            dummy_output = self.conv3d(dummy_input)\n",
        "            self.flattened_size = dummy_output.view(1, -1).shape[1]\n",
        "        self.fc = nn.Linear(self.flattened_size, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # add channel dim\n",
        "        x = self.conv3d(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class GCNBranch(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 64)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.fc = nn.Linear(128, 128)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.3, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class DualBranchFusion(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.branch1 = SpectralSpatialCNN(in_channels, num_classes)\n",
        "        self.branch2 = GCNBranch(in_channels, num_classes)\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x_img, x_graph_all, edge_index, batch_indices):\n",
        "        f1 = self.branch1(x_img)\n",
        "        f2_all = self.branch2(x_graph_all, edge_index)\n",
        "        f2 = f2_all[batch_indices]\n",
        "        fused = torch.cat((f1, f2), dim=1)\n",
        "        out = self.classifier(fused)\n",
        "        return out\n",
        "\n",
        "# 3. Metrics\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    return acc, f1, kappa\n"
      ],
      "metadata": {
        "id": "eVo_D2L_30mJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Dataset class\n",
        "class IndianPinesDataset(Dataset):\n",
        "    def __init__(self, x_img, x_graph, y):\n",
        "        self.x_img = x_img\n",
        "        self.x_graph = x_graph\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x_img[idx], self.x_graph[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "BhcOq5V439M9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Main\n",
        "if __name__ == '__main__':\n",
        "    data, labels = load_indian_pines()\n",
        "    data = preprocess(data)\n",
        "    patches, targets, indices = extract_patches(data, labels)\n",
        "\n",
        "    patches = patches.reshape((patches.shape[0], -1, patches.shape[-1]))\n",
        "    le = LabelEncoder()\n",
        "    targets = le.fit_transform(targets)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    x_graph = torch.tensor(scaler.fit_transform(patches[:, patches.shape[1]//2]), dtype=torch.float32)\n",
        "\n",
        "    adj = kneighbors_graph(x_graph.numpy(), n_neighbors=8, mode='connectivity', include_self=False)\n",
        "    edge_index_np = dense_to_sparse(torch.tensor(adj.toarray(), dtype=torch.float))[0]\n",
        "    edge_index = edge_index_np.long()\n",
        "\n",
        "    num_samples = patches.shape[0]\n",
        "    x_img = torch.tensor(patches[:, :, :].reshape(num_samples, 5, 5, data.shape[2]), dtype=torch.float32)\n",
        "    y = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "    indices = np.arange(num_samples)\n",
        "    train_idx, test_idx = train_test_split(indices, test_size=0.3, random_state=42)\n",
        "\n",
        "    x_train, x_test = x_img[train_idx], x_img[test_idx]\n",
        "    xg_train, xg_test = x_graph[train_idx], x_graph[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "    train_dataset = IndianPinesDataset(x_train, xg_train, y_train)\n",
        "    test_dataset = IndianPinesDataset(x_test, xg_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    train_mask = torch.zeros(num_samples, dtype=torch.bool)\n",
        "    train_mask[train_idx] = True\n",
        "    edge_mask = train_mask[edge_index[0]] & train_mask[edge_index[1]]\n",
        "    edge_index_train = edge_index[:, edge_mask]\n",
        "\n",
        "    model = DualBranchFusion(in_channels=data.shape[2], num_classes=len(np.unique(targets))).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    for epoch in range(100):\n",
        "        epoch_loss = 0.0\n",
        "        x_graph_all = x_graph.to(device)\n",
        "        edge_index_train = edge_index_train.to(device)\n",
        "\n",
        "        for x_img_batch, xg_batch, y_batch in train_loader:\n",
        "            x_img_batch = x_img_batch.to(device)\n",
        "            xg_batch = xg_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            batch_indices = (xg_batch[:, None] == x_graph_all).all(dim=2).nonzero(as_tuple=True)[1]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x_img_batch, x_graph_all, edge_index_train, batch_indices)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNg8Trx03_pw",
        "outputId": "275effe7-cc60-493e-8e29-77b78b8a9461"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 3.3884\n",
            "Epoch 2, Loss: 1.3339\n",
            "Epoch 3, Loss: 1.2687\n",
            "Epoch 4, Loss: 1.0368\n",
            "Epoch 5, Loss: 0.9284\n",
            "Epoch 6, Loss: 0.7507\n",
            "Epoch 7, Loss: 0.6680\n",
            "Epoch 8, Loss: 0.6975\n",
            "Epoch 9, Loss: 0.5555\n",
            "Epoch 10, Loss: 0.5355\n",
            "Epoch 11, Loss: 0.4809\n",
            "Epoch 12, Loss: 0.4624\n",
            "Epoch 13, Loss: 0.4281\n",
            "Epoch 14, Loss: 0.4154\n",
            "Epoch 15, Loss: 0.4234\n",
            "Epoch 16, Loss: 0.4067\n",
            "Epoch 17, Loss: 0.3808\n",
            "Epoch 18, Loss: 0.3673\n",
            "Epoch 19, Loss: 0.3392\n",
            "Epoch 20, Loss: 0.3423\n",
            "Epoch 21, Loss: 0.3401\n",
            "Epoch 22, Loss: 0.3322\n",
            "Epoch 23, Loss: 0.3110\n",
            "Epoch 24, Loss: 0.3301\n",
            "Epoch 25, Loss: 0.2858\n",
            "Epoch 26, Loss: 0.3027\n",
            "Epoch 27, Loss: 0.2775\n",
            "Epoch 28, Loss: 0.2878\n",
            "Epoch 29, Loss: 0.2610\n",
            "Epoch 30, Loss: 0.2684\n",
            "Epoch 31, Loss: 0.2542\n",
            "Epoch 32, Loss: 0.2543\n",
            "Epoch 33, Loss: 0.2583\n",
            "Epoch 34, Loss: 0.2415\n",
            "Epoch 35, Loss: 0.2304\n",
            "Epoch 36, Loss: 0.2165\n",
            "Epoch 37, Loss: 0.2504\n",
            "Epoch 38, Loss: 0.2521\n",
            "Epoch 39, Loss: 0.2516\n",
            "Epoch 40, Loss: 0.2180\n",
            "Epoch 41, Loss: 0.2015\n",
            "Epoch 42, Loss: 0.2117\n",
            "Epoch 43, Loss: 0.1821\n",
            "Epoch 44, Loss: 0.2164\n",
            "Epoch 45, Loss: 0.2030\n",
            "Epoch 46, Loss: 0.2114\n",
            "Epoch 47, Loss: 0.1918\n",
            "Epoch 48, Loss: 0.1719\n",
            "Epoch 49, Loss: 0.1848\n",
            "Epoch 50, Loss: 0.1762\n",
            "Epoch 51, Loss: 0.1709\n",
            "Epoch 52, Loss: 0.1716\n",
            "Epoch 53, Loss: 0.1608\n",
            "Epoch 54, Loss: 0.1521\n",
            "Epoch 55, Loss: 0.1418\n",
            "Epoch 56, Loss: 0.1357\n",
            "Epoch 57, Loss: 0.1734\n",
            "Epoch 58, Loss: 0.1625\n",
            "Epoch 59, Loss: 0.1182\n",
            "Epoch 60, Loss: 0.1374\n",
            "Epoch 61, Loss: 0.1343\n",
            "Epoch 62, Loss: 0.1291\n",
            "Epoch 63, Loss: 0.1632\n",
            "Epoch 64, Loss: 0.1338\n",
            "Epoch 65, Loss: 0.1260\n",
            "Epoch 66, Loss: 0.1652\n",
            "Epoch 67, Loss: 0.1158\n",
            "Epoch 68, Loss: 0.1214\n",
            "Epoch 69, Loss: 0.1249\n",
            "Epoch 70, Loss: 0.1128\n",
            "Epoch 71, Loss: 0.0999\n",
            "Epoch 72, Loss: 0.0954\n",
            "Epoch 73, Loss: 0.0921\n",
            "Epoch 74, Loss: 0.1057\n",
            "Epoch 75, Loss: 0.0978\n",
            "Epoch 76, Loss: 0.1283\n",
            "Epoch 77, Loss: 0.1018\n",
            "Epoch 78, Loss: 0.0819\n",
            "Epoch 79, Loss: 0.0809\n",
            "Epoch 80, Loss: 0.0852\n",
            "Epoch 81, Loss: 0.1084\n",
            "Epoch 82, Loss: 0.0775\n",
            "Epoch 83, Loss: 0.0831\n",
            "Epoch 84, Loss: 0.1147\n",
            "Epoch 85, Loss: 0.0879\n",
            "Epoch 86, Loss: 0.0895\n",
            "Epoch 87, Loss: 0.1025\n",
            "Epoch 88, Loss: 0.0793\n",
            "Epoch 89, Loss: 0.1019\n",
            "Epoch 90, Loss: 0.0725\n",
            "Epoch 91, Loss: 0.0702\n",
            "Epoch 92, Loss: 0.0674\n",
            "Epoch 93, Loss: 0.0730\n",
            "Epoch 94, Loss: 0.0596\n",
            "Epoch 95, Loss: 0.0861\n",
            "Epoch 96, Loss: 0.1158\n",
            "Epoch 97, Loss: 0.0764\n",
            "Epoch 98, Loss: 0.0861\n",
            "Epoch 99, Loss: 0.0790\n",
            "Epoch 100, Loss: 0.0668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Evaluation\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x_graph_all = x_graph.to(device)\n",
        "        edge_index_train = edge_index_train.to(device)\n",
        "\n",
        "        for x_img_batch, xg_batch, y_batch in test_loader:\n",
        "            x_img_batch = x_img_batch.to(device)\n",
        "            xg_batch = xg_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            batch_indices = (xg_batch[:, None] == x_graph_all).all(dim=2).nonzero(as_tuple=True)[1]\n",
        "            outputs = model(x_img_batch, x_graph_all, edge_index_train, batch_indices)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "    y_true = np.array(all_labels)\n",
        "    y_pred = np.array(all_preds)\n",
        "\n",
        "    oa = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"Overall Accuracy (OA): {oa:.4f}\")\n",
        "    print(f\"F1 Score (Macro): {f1:.4f}\")\n",
        "    print(f\"Cohen's Kappa: {kappa:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roavDU0a4FwF",
        "outputId": "c0b7e6d3-bdd2-41d0-ce29-be6d5a6d60a5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy (OA): 0.9145\n",
            "F1 Score (Macro): 0.9236\n",
            "Cohen's Kappa: 0.9026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Training Set Evaluation\n",
        "    model.eval()\n",
        "    all_train_preds, all_train_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x_graph_all = x_graph.to(device)\n",
        "        edge_index_train = edge_index_train.to(device)\n",
        "\n",
        "        for x_img_batch, xg_batch, y_batch in train_loader:\n",
        "            x_img_batch = x_img_batch.to(device)\n",
        "            xg_batch = xg_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            batch_indices = (xg_batch[:, None] == x_graph_all).all(dim=2).nonzero(as_tuple=True)[1]\n",
        "            outputs = model(x_img_batch, x_graph_all, edge_index_train, batch_indices)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "\n",
        "            all_train_preds.extend(preds.cpu().numpy())\n",
        "            all_train_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "    y_train_true = np.array(all_train_labels)\n",
        "    y_train_pred = np.array(all_train_preds)\n",
        "\n",
        "    train_oa = accuracy_score(y_train_true, y_train_pred)\n",
        "    train_f1 = f1_score(y_train_true, y_train_pred, average='macro')\n",
        "    train_kappa = cohen_kappa_score(y_train_true, y_train_pred)\n",
        "\n",
        "    print(f\"\\nTraining Accuracy (OA): {train_oa:.4f}\")\n",
        "    print(f\"Training F1 Score (Macro): {train_f1:.4f}\")\n",
        "    print(f\"Training Cohen's Kappa: {train_kappa:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGZoa5vS5VNz",
        "outputId": "5021b6ef-f787-4971-e202-ecdbb9037b54"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Accuracy (OA): 0.9939\n",
            "Training F1 Score (Macro): 0.9975\n",
            "Training Cohen's Kappa: 0.9930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJ2707fV5m6g"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}